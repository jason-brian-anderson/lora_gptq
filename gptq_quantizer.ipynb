{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78437d00-0dfd-420f-9d1d-accf9a2cde82",
   "metadata": {},
   "source": [
    "The goal of this notebook is to distill down the essential compenents of training HF models in a way that is easy to port to a data pipeline and apply to new model training scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4105b-66b9-43ed-81d8-9769e56f1df8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5186aef3-01b0-4a1f-ad6e-9c99d46ca824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datetime import datetime;\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f647f4-e46c-4b7d-bb28-4e11e6d632ab",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88eb322b-c949-4461-9d04-7e4d396d3e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023_11_11'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if True:\n",
    "    date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85d9c78-ef0c-4d91-a9df-bd9c799da50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work/work_for_2023_11_11/best_checkpoint'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_dir = \"work\"\n",
    "seed = 42 #reproducability  over training runs\n",
    "clobber_project_dir = False #wipe the project director clean\n",
    "project_dir = f\"work_for_{date_str}\" #project name, also used as wandb project name\n",
    "\n",
    "#creds\n",
    "hf_token = os.environ[\"HF_TOKEN\"]\n",
    "wandb_key = os.environ[\"WANDB_KEY\"]\n",
    "hf_account_name = os.environ[\"HF_ACCOUNT_NAME\"]\n",
    "\n",
    "#HF touchpoints\n",
    "hf_dataset_name = \"knkarthick/dialogsum\"\n",
    "source_hf_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "checkpoint_dir = \"best_checkpoint\"\n",
    "quantization_dir = 'quantized_8bit'\n",
    "merged_dir = 'merged_model'\n",
    "quantized_train_dir = 'quant_trained_model_new'\n",
    "\n",
    "train_test_split_ratio = 0.025\n",
    "\n",
    "#LoRA hparams\n",
    "lora_r = 16  # rank\n",
    "lora_alpha = lora_r * 2\n",
    "lora_dropout = 0.05\n",
    "\n",
    "#training hparams\n",
    "epochs = 1\n",
    "#max_steps = 500\n",
    "per_device_train_batch_size=16\n",
    "gradient_accumulation_steps=16\n",
    "early_stopping_patience = 3\n",
    "learning_rate = 7e-4\n",
    "logging_steps = 5\n",
    "device = \"cuda\"\n",
    "max_training_sample_length = 512 #2048\n",
    "\n",
    "#quantization params\n",
    "quantization_bits = 4\n",
    "\n",
    "destination_hf_model_name = f\"{hf_account_name}/llama2-7b-dialogsum-qlora-gptq\"\n",
    "project_path = os.path.join(work_dir,project_dir, checkpoint_dir)\n",
    "quant_path = os.path.join(project_dir, quantization_dir)\n",
    "quantized_train_path = os.path.join(project_dir, quantized_train_dir)\n",
    "merged_path = os.path.join(project_path, merged_dir)\n",
    "\n",
    "project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0fc97-fbe3-4097-9751-3feafcbf218b",
   "metadata": {},
   "source": [
    "## State of working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ffbb7b-ca53-4226-a31f-407e141bc587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">work_for_2023_11_11\n",
       "</pre>\n"
      ],
      "text/plain": [
       "work_for_2023_11_11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "0 drwxr-xr-x 1 root root 4096 Nov 11 19:03 .\n",
      "0 drwxr-xr-x 1 root root 4096 Nov 11 19:03 runs\n",
      "0 drwxr-xr-x 1 root root 4096 Nov 11 19:38 ..\n"
     ]
    }
   ],
   "source": [
    "print(project_dir)\n",
    "!mkdir -p $project_dir\n",
    "\n",
    "#Set clobber_project_dir to True to reset working directory\n",
    "if clobber_project_dir:\n",
    "    !rm -rf ./$project_dir\n",
    "\n",
    "!mkdir -p $project_dir\n",
    "\n",
    "!ls -latrs ./$project_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c44be-3cb1-45d4-ba26-5fa6a1e39e46",
   "metadata": {},
   "source": [
    "# Process data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274adcda-c2a1-4524-b964-75b69186daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def analyze_generation_params(text, model):\n",
    "    print(text,\"-\"*60)\n",
    "    results = []\n",
    "    for temp in np.linspace(.5,1.5,num = 5):\n",
    "        print(f\"temp {temp}:\",end=\"\")\n",
    "        results.append(generate(text, model, prompt=False, max_new_tokens=185,temp=temp))\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    results = []\n",
    "    for top_p in np.linspace(0,1,num = 11):\n",
    "        print(f\"top_p {top_p}:\",end=\"\")\n",
    "        results.append(generate(text, model, prompt=False, max_new_tokens=185,temp=.7, top_p=top_p))\n",
    "    \n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    results = []\n",
    "    for top_k in np.linspace(0,100,num = 11,dtype=int):\n",
    "        print(f\"top_k {top_k}:\",end=\"\")\n",
    "        results.append(generate(text,  model, prompt=False, max_new_tokens=185, top_k=int(top_k)))\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    results = []\n",
    "    for do_sample in [True,False]:\n",
    "        print(f\"do_sample {do_sample}:\",end=\"\")\n",
    "        results.append(generate(text,  model, prompt=False, max_new_tokens=185, do_sample = do_sample, top_k=None))\n",
    "    \n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    results = []\n",
    "    for repetition_penalty in np.linspace(1.,2.,num=11):\n",
    "        print(f\"repetition_penalty {repetition_penalty}:\",end=\"\")\n",
    "        results.append(generate(text,  model, prompt=False, max_new_tokens=48,repetition_penalty=repetition_penalty))\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    results = []\n",
    "    for typical_p in np.linspace(.1,.9,num=9):\n",
    "        print(f\"typical_p {typical_p}:\",end=\"\")\n",
    "        results.append(generate(text,  model, prompt=False, max_new_tokens=48,typical_p=typical_p))\n",
    "\n",
    "\n",
    "def generate(text, model, prompt=False, temp=0.7,top_k= 40, top_p = 0.1, do_sample = True, repetition_penalty = 1.23, typical_p = 1, guidance_scale = 1, max_new_tokens = 24,):\n",
    "    from transformers import TextStreamer\n",
    "    if prompt:\n",
    "        print(text,\"-\"*20)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    # Call the generate method of the model with the given inputs and additional generation configurations.\n",
    "    outputs = model.generate(**inputs,  # \"inputs\" likely includes input_ids or prompts for generation.\n",
    "                           streamer=streamer,  # \"streamer\" not a standard parameter in HF documentation, likely custom for model-specific streaming.\n",
    "                           max_new_tokens=max_new_tokens,  # Sets the maximum number of new tokens to generate; range varies based on model and computational limits.\n",
    "                           temperature=temp,  # Controls randomness: lower->more deterministic, higher->more random; typically in range [0.5, 1.5].\n",
    "                           do_sample=do_sample,  # Enables sampling; when True, picks tokens based on probability distribution, rather than just most likely.\n",
    "                           top_p=top_p,  # Nucleus sampling: selects top p% probability tokens for sampling; range [0, 1].\n",
    "                           top_k=top_k,  # Top-k sampling: chooses from top k probability tokens; if k=0, it's the same as using no top-k.\n",
    "                           repetition_penalty=repetition_penalty,  # Penalizes repeated tokens; >1 discourages, <1 encourages repetition; typically close to 1.\n",
    "                           typical_p=typical_p,  # Typical sampling; selects tokens whose cumulative probability is above this threshold, range [0, 1], usually close to 1.\n",
    "                           guidance_scale=guidance_scale,  # Affects the scale of guidance in models that support it, like CTRL; standard range not well-defined.\n",
    "                           #seed=seed,  # Sets a seed for reproducibility; commented out, so not in use.\n",
    "                          )\n",
    "    generated_tokens = outputs[0].tolist()[len(inputs[0]):]\n",
    "    result = tokenizer.decode(generated_tokens)\n",
    "    return result\n",
    "\n",
    "def set_plot_appearance(fontsize_title=14, \n",
    "                        fontsize_label=12, \n",
    "                        fontsize_ticks=8, \n",
    "                        theme='dark'):\n",
    "    \"\"\"\n",
    "    Configure the plot's appearance with font sizes and theme for dark backgrounds.\n",
    "    \n",
    "    Parameters:\n",
    "    fontsize_title: Font size for the title.\n",
    "    fontsize_label: Font size for the x and y labels.\n",
    "    fontsize_ticks: Font size for the x and y tick labels.\n",
    "    theme: Seaborn theme for the plot's aesthetic style.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=theme, palette='pastel')\n",
    "    \n",
    "    plt.rcParams.update({\n",
    "        'figure.facecolor': '#1a1a1a',  # Very dark grey background for the figure\n",
    "        'axes.facecolor': '#1a1a1a',    # Very dark grey background for the plots\n",
    "        'grid.color': 'gray',           # Lighter grey grid lines (less contrast)\n",
    "        'text.color': 'white',          # White text for better contrast on dark bg\n",
    "        'axes.labelcolor': 'white',     # White labels for axes\n",
    "        'xtick.color': 'white',         # White x-tick labels\n",
    "        'ytick.color': 'white',         # White y-tick labels\n",
    "        'axes.labelsize': fontsize_label,\n",
    "        'axes.titlesize': fontsize_title,\n",
    "        'xtick.labelsize': fontsize_ticks,\n",
    "        'ytick.labelsize': fontsize_ticks,\n",
    "        'legend.title_fontsize': fontsize_ticks + 2,\n",
    "        'legend.fontsize': fontsize_ticks,\n",
    "        'axes.edgecolor': 'lightgrey',  # Light grey color for the axes' spines\n",
    "    })\n",
    "# Possible modern themes: 'darkgrid', 'whitegrid', 'dark', 'white', and 'ticks'.\n",
    "# Example usage: set_plot_appearance(theme='ticks')\n",
    "\n",
    "def plot_text_length_histogram(df, bucket_size=64, percentile=95):\n",
    "    \"\"\"\n",
    "    Plot a histogram of text lengths and indicate the percentile.\n",
    "\n",
    "    Parameters:\n",
    "    df: the pandas dataframe to render from\n",
    "    bucket_size: the number of characters to bucket the histogram with\n",
    "    percentile: draws a vertical line at this percentile to visualize whether an acceptable fraction of the data lenghts are being used for training set\n",
    "    \"\"\"\n",
    "    set_plot_appearance()  # Apply appearance settings to the plot\n",
    "    text_lengths = df['text'].apply(len)  # Calculate text lengths.\n",
    "    bins = range(0, max(text_lengths) + bucket_size, bucket_size)  # Define histogram bins.\n",
    "    percentile_value = np.percentile(text_lengths, percentile)  # Calculate percentile value.\n",
    "\n",
    "    fig, ax = plt.subplots()  # Create figure and axis objects.\n",
    "    ax.hist(text_lengths, bins=bins, edgecolor='black', alpha=0.7)  # Plot histogram.\n",
    "    ax.axvline(x=percentile_value, color='red', linestyle='--')  # Draw percentile line.\n",
    "    \n",
    "    # Annotate the percentile line.\n",
    "    ax.text(percentile_value, ax.get_ylim()[1]*0.9, f' {percentile}th Percentile: {int(round(percentile_value, -1))}',\n",
    "            color='red', ha='left')\n",
    "\n",
    "    ax.set_xlabel('Text Length')  # Set x-axis label.\n",
    "    ax.set_ylabel('Frequency')  # Set y-axis label.\n",
    "    ax.set_title('Histogram of Text Lengths')  # Set title.\n",
    "\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set major locator for x-axis.\n",
    "    plt.show()  # Display the plot.\n",
    "\n",
    "def visualize_train_test_category_splits(train_df, test_df, top_x=5, normalize=True):\n",
    "    \"\"\"\n",
    "    Visualize the frequency of top categories in training and test datasets.\n",
    "    \"\"\"\n",
    "    set_plot_appearance()  # Apply appearance settings to the plot\n",
    "\n",
    "    # Get top x categories from both datasets.\n",
    "    train_topic_norm = train_df['topic'].value_counts(normalize=normalize).head(top_x)\n",
    "    test_topic_norm = test_df['topic'].value_counts(normalize=normalize).head(top_x)\n",
    "    \n",
    "    # Prepare the data for plotting.\n",
    "    train_topics = pd.DataFrame({'topic': train_topic_norm.index, 'frequency': train_topic_norm.values, 'dataset': 'Training'})\n",
    "    test_topics = pd.DataFrame({'topic': test_topic_norm.index, 'frequency': test_topic_norm.values, 'dataset': 'Test'})\n",
    "    combined_topics = pd.concat([train_topics, test_topics])\n",
    "\n",
    "    sns.barplot(x='frequency', y='topic', hue='dataset', data=combined_topics)  # Create bar plot.\n",
    "    plt.title('Top {} Topics Frequency Comparison'.format(top_x))  # Set title.\n",
    "    plt.xlabel('Normalized Frequency')  # Set x-axis label.\n",
    "    plt.ylabel('Topic')  # Set y-axis label.\n",
    "    plt.tight_layout()  # Adjust layout.\n",
    "    plt.show()  # Display the plot.\n",
    "\n",
    "\n",
    "\n",
    "def get_data_stuff(dataset_name, split_ratio=0.2):    \n",
    "    # %%\n",
    "    from datasets import load_dataset\n",
    "    from datasets import Dataset\n",
    "    import pandas as pd\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    \n",
    "    #load the dataset\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    \n",
    "    # Shuffle the dataset (setting seed for reproducibility)\n",
    "    shuffled_ds = dataset['train']#.shuffle(seed=seed)\n",
    "    #shuffled_ds = shuffled_ds.select(range(1000))\n",
    "\n",
    "    \n",
    "    # Split the dataset into training and test sets with a test size of 20%\n",
    "    train_test_split = shuffled_ds.train_test_split(test_size=train_test_split_ratio, seed = seed)\n",
    "    \n",
    "    # The train/test datasets are now accessible as follows:\n",
    "    train_ds = train_test_split['train']\n",
    "    raw_train_df = pd.DataFrame(train_ds)\n",
    "    \n",
    "    test_ds = train_test_split['test']\n",
    "    raw_test_df = pd.DataFrame(test_ds)\n",
    "    \n",
    "    prepared_train_df = prepare_dataset(raw_train_df, \"train\")\n",
    "    prepared_test_df = prepare_dataset(raw_test_df, \"test\")\n",
    "\n",
    "    prepared_train_dataset = Dataset.from_pandas(prepared_train_df)\n",
    "    prepared_test_dataset = Dataset.from_pandas(prepared_test_df)\n",
    "    \n",
    "    print(f\"length of downloaded ds: {len(shuffled_ds)}\")\n",
    "    print(f\"length of train_ds: {len(prepared_train_dataset)}\")\n",
    "    print(f\"length of test_ds: {len(prepared_test_dataset)}\")\n",
    "    estimated_training_steps = epochs * len(prepared_train_dataset) / per_device_train_batch_size / gradient_accumulation_steps\n",
    "    print(f\"estimated training steps will be {estimated_training_steps}\")\n",
    "    return prepared_train_df, prepared_test_df, prepared_train_dataset, prepared_test_dataset\n",
    "\n",
    "\n",
    "def prepare_dataset(df, split=\"train\"):\n",
    "    text_col = []\n",
    "    instruction = \"\"\"Write a concise summary of the below input text.Ensure that responses covers the key points of the text.Only provide full sentence responses.\"\"\"  # change instuction according to the task\n",
    "    if split == \"train\":\n",
    "        for _, row in df.iterrows():\n",
    "            input_q = row[\"dialogue\"]\n",
    "            output = row[\"summary\"]\n",
    "            text = (\n",
    "                \"### Instruction: \\n\"\n",
    "                + instruction\n",
    "                + \"\\n### Input: \\n\"\n",
    "                + input_q\n",
    "                + \"\\n### Response :\\n\"\n",
    "                + output\n",
    "                + \"\\n### End\"\n",
    "            )  # keeping output column in training dataset\n",
    "            text_col.append(text)\n",
    "        df.loc[:, \"text\"] = text_col\n",
    "    else:\n",
    "        for _, row in df.iterrows():\n",
    "            input_q = row[\"dialogue\"]\n",
    "            text = (\n",
    "                \"### Instruction: \\n\"\n",
    "                + instruction\n",
    "                + \"\\n### Input: \\n\"\n",
    "                + input_q\n",
    "                + \"\\n### Response :\\n\"\n",
    "            )  # not keeping output column in test dataset\n",
    "            text_col.append(text)\n",
    "        df.loc[:, \"text\"] = text_col\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a764bbf-2d24-410d-9356-68f2153af13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">length of downloaded ds: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12460</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "length of downloaded ds: \u001b[1;36m12460\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">length of train_ds: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12148</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "length of train_ds: \u001b[1;36m12148\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">length of test_ds: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">312</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "length of test_ds: \u001b[1;36m312\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">estimated training steps will be <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47.453125</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "estimated training steps will be \u001b[1;36m47.453125\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df, test_df, train_dataset, test_dataset = get_data_stuff(hf_dataset_name, \n",
    "                                                                split_ratio=train_test_split_ratio,\n",
    "                                                               )\n",
    "text = test_df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b9d045-f3df-45b3-beea-92336c089d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">### Instruction: \n",
       "Write a concise summary of the below input text.Ensure that responses covers the key points of the text.Only \n",
       "provide full sentence responses.\n",
       "### Input: \n",
       "#Person1#: Mister Ewing said we should show up at the conference center at <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">4:00</span> o'clock, right?\n",
       "#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are \n",
       "coming, and he wants to make a good impression on them. How are you getting there?\n",
       "#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is \n",
       "construction on the highway. What about you?\n",
       "#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center \n",
       "only once, and I'm not sure if I can find my way around there.\n",
       "### Response :\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "### Instruction: \n",
       "Write a concise summary of the below input text.Ensure that responses covers the key points of the text.Only \n",
       "provide full sentence responses.\n",
       "### Input: \n",
       "#Person1#: Mister Ewing said we should show up at the conference center at \u001b[1;92m4:00\u001b[0m o'clock, right?\n",
       "#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are \n",
       "coming, and he wants to make a good impression on them. How are you getting there?\n",
       "#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is \n",
       "construction on the highway. What about you?\n",
       "#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center \n",
       "only once, and I'm not sure if I can find my way around there.\n",
       "### Response :\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d05cc5-123a-4e12-82cd-57671df3a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "# loading the model with quantization config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    source_hf_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map= \"auto\",\n",
    "    #token = hf_token,\n",
    ")\n",
    "\n",
    "model.config.use_cache = True  #surpress the download and use cache. # apparently needed because of https://github.com/huggingface/transformers/pull/24906\n",
    "model.config.pretraining_tp = 1  #disable tensor parallelism\n",
    "\n",
    "#fixme\n",
    "tokenizer = AutoTokenizer.from_pretrained(source_hf_model_name,\n",
    "     trust_remote_code=True, \n",
    "     return_token_type_ids=False,\n",
    "     token = hf_token,\n",
    " )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0253b-48f1-4f6f-824b-762c857d22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40757681-3f73-4b95-9e14-9228c67b668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import trl\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "import transformers\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    #target_modules=[ \"q_proj\",\"v_proj\",],  # we will only create adopters for q, v metrices of attention module\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"],\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    output_dir=project_dir,\n",
    "    #load_from_checkpoint=str,\n",
    "    seed = seed,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate= learning_rate,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    #max_steps=max_steps,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    evaluation_strategy=\"steps\",  # Add this line\n",
    "    eval_steps=logging_steps,  # Add this line\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,  # passing peft config\n",
    "    dataset_text_field=\"text\",  # mentioned the required column\n",
    "    args=training_arguments,  # training agruments\n",
    "    tokenizer=tokenizer,  # tokenizer\n",
    "    packing=False,\n",
    "    max_seq_length=max_training_sample_length,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
    ")\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bfc5b-da5b-4f93-9b5b-e9e48097f947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if True:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d4322-25cf-4a33-92a2-a23eab8439b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40dedb5-c6af-47b8-936f-fbf5b16cec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "trainer.model.save_pretrained(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d58d58-3ba8-44e8-b6fc-a51154a9af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0d830-d825-41f4-91b6-d7e424844dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lhatrs ./$project_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7faf6-3e0c-4f5c-922d-cabd545bac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -latrsh ./$project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787bccd7-4a58-4cfa-a3fc-3a30d0a9b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./$project_path/README.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f030a0e-f519-42bc-b1fb-8c11b2bd59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig.from_pretrained(project_path)\n",
    "loaded_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869dacf-b6b2-4ad1-942d-b5336e4606c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = test_df['text'][500]\n",
    "print(text)\n",
    "result = generate(text, loaded_model, max_new_tokens=64, temp=0.1)\n",
    "#del lmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bb12c-2f64-44b6-94a5-c52e82ba7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del  loaded_model\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974716d4-a8d1-4ea0-a43a-64d2a063f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68debd1-1547-4210-b56c-4bf51ce35621",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(project_path)\n",
    "!ls -latrsh $project_path\n",
    "!cat $project_path/adapter_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcd0ff-edd5-42c7-8b7f-63e91fea44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    source_hf_model_name, \n",
    "    trust_remote_code=True, \n",
    "    return_token_type_ids=False,\n",
    "    token = hf_token,\n",
    ")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(source_hf_model_name)\n",
    "\n",
    "persisted_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    project_path, # path to the project or model directory\n",
    "    low_cpu_mem_usage=True, # reduces CPU memory usage if True\n",
    "    return_dict=True, # ensures output is returned as a dictionary\n",
    "    torch_dtype=torch.float16, # sets the data type for PyTorch tensors\n",
    "    device_map=device, # specifies the device placement for the model\n",
    "    #token=hf_token, # token for Hugging Face authentication\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2a7b5-7e32-443e-8eee-01a45e50c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = test_df['text'][500]\n",
    "print(text)\n",
    "result = generate(text, persisted_model, max_new_tokens=96, temp=0.1)\n",
    "#del persisted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5eca4a-dbf3-463c-97e1-b2df66da15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fd3c3-140e-4f8d-8594-dfd01c8d6acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ae65f-46f8-45d6-94da-61edc884dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Merges the parts of the model that were possibly loaded on different devices and unloads them from memory\n",
    "# ^^ from gpt4.  i suspect an halucination, verify.\n",
    "merged_model = persisted_model.merge_and_unload(progressbar=True)\n",
    "\n",
    "merged_model.save_pretrained(merged_path, safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "del persisted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d50514-acd2-48a5-abc3-ec86c29673de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_path)\n",
    "!ls -lathrs $merged_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6b94e-4273-4cd2-8703-d268dfb5a001",
   "metadata": {},
   "source": [
    "### if dead again, try starting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8de00f-b0e1-4056-bd95-d74f910f1b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278c014-3120-443d-9713-405b44f4003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#qqqq\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_dir)\n",
    "merged_model_from_disk = AutoModelForCausalLM.from_pretrained( merged_dir, \n",
    "                                                              device_map=device,\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0bcf5-3eb4-4858-9f27-634daeb486a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(text)\n",
    "result = generate(text, merged_model_from_disk,max_new_tokens=96, temp = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469bf85-7598-404e-9629-7a04ee4788c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del merged_model_from_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891dd5c-bd1a-4df5-85f5-956af0f6e2e4",
   "metadata": {},
   "source": [
    "## Quantize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0fdab5-15b8-43b2-8d00-b0bac153833c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work/work_for_2023_11_11/best_checkpoint/merged_model'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7373ac2-d526-4190-ae7a-cdf7a98e0dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996f02ddf0e24c1fb97ba7af23e4a680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bf5bdacce94502912259b82a217720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 43s, sys: 2min 27s, total: 39min 10s\n",
      "Wall time: 19min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import GPTQConfig\n",
    "\n",
    "quantization_config = GPTQConfig(\n",
    "    bits=quantization_bits,\n",
    "    dataset=[\"c4\"],\n",
    "    desc_act=False,\n",
    ")\n",
    "\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_path, \n",
    "    quantization_config=quantization_config, \n",
    "    device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e35e993-b47b-42c8-b683-4c243f693d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del persisted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5297fcab-e846-42d4-a32f-e233dd673d30",
   "metadata": {},
   "source": [
    "## Save quantized model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc207ed-e2a3-4af9-920c-7c447a2bd2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work_for_2023_11_11/quantized_8bit'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8431321-8378-42e4-b14b-81ab9fbd31ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">saving to work_for_2023_11_11/quantized_8bit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "saving to work_for_2023_11_11/quantized_8bit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 862 ms, total: 2.16 s\n",
      "Wall time: 5.21 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('work_for_2023_11_11/quantized_8bit/tokenizer_config.json',\n",
       " 'work_for_2023_11_11/quantized_8bit/special_tokens_map.json',\n",
       " 'work_for_2023_11_11/quantized_8bit/tokenizer.model',\n",
       " 'work_for_2023_11_11/quantized_8bit/added_tokens.json',\n",
       " 'work_for_2023_11_11/quantized_8bit/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "quant_path = os.path.join(project_dir, quantization_dir)\n",
    "print(f\"saving to {quant_path}\")\n",
    "\n",
    "# Save the quantized model\n",
    "quant_model.save_pretrained(quant_path, safe_serialization=True)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "#del persisted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c63e7173-bce1-42d7-8b38-cd1684247665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">done\n",
       "</pre>\n"
      ],
      "text/plain": [
       "done\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d01927a-4ef2-4718-8f03-34caa6c19172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">work_for_2023_11_11/quantized_8bit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "work_for_2023_11_11/quantized_8bit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.7G\n",
      "   0 drwxr-xr-x 1 root root 4.0K Nov 11 19:58 ..\n",
      "4.0K -rw-r--r-- 1 root root 1.2K Nov 11 19:58 config.json\n",
      "   0 -rw-r--r-- 1 root root  183 Nov 11 19:58 generation_config.json\n",
      "3.7G -rw-r--r-- 1 root root 3.7G Nov 11 19:58 model.safetensors\n",
      "4.0K -rw-r--r-- 1 root root  900 Nov 11 19:58 tokenizer_config.json\n",
      "   0 -rw-r--r-- 1 root root  414 Nov 11 19:58 special_tokens_map.json\n",
      "492K -rw-r--r-- 1 root root 489K Nov 11 19:58 tokenizer.model\n",
      "   0 drwxr-xr-x 1 root root 4.0K Nov 11 19:58 .\n",
      "1.8M -rw-r--r-- 1 root root 1.8M Nov 11 19:58 tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "print(quant_path)\n",
    "!ls -latrsh $quant_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad76bea-cf16-49eb-aa88-41f7a4aa8b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d21bcd6-7817-48d2-965a-4fdd56b22d28",
   "metadata": {},
   "source": [
    "## Load quantized model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d47bde57-c8ba-4667-95ad-5d00ddf8b2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">work_for_2023_11_11/quantized_8bit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "work_for_2023_11_11/quantized_8bit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.48 s, sys: 8.3 s, total: 14.8 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(quant_path)\n",
    "quant_model_from_disk = AutoModelForCausalLM.from_pretrained( quant_path, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355987cd-30e6-4190-b6f2-c9c580869f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">### Instruction: \n",
       "Write a concise summary of the below input text.Ensure that responses covers the key points of the text.Only \n",
       "provide full sentence responses.\n",
       "### Input: \n",
       "#Person1#: Mister Ewing said we should show up at the conference center at <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">4:00</span> o'clock, right?\n",
       "#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are \n",
       "coming, and he wants to make a good impression on them. How are you getting there?\n",
       "#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is \n",
       "construction on the highway. What about you?\n",
       "#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center \n",
       "only once, and I'm not sure if I can find my way around there.\n",
       "### Response :\n",
       " \n",
       " ==============================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "### Instruction: \n",
       "Write a concise summary of the below input text.Ensure that responses covers the key points of the text.Only \n",
       "provide full sentence responses.\n",
       "### Input: \n",
       "#Person1#: Mister Ewing said we should show up at the conference center at \u001b[1;92m4:00\u001b[0m o'clock, right?\n",
       "#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are \n",
       "coming, and he wants to make a good impression on them. How are you getting there?\n",
       "#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is \n",
       "construction on the highway. What about you?\n",
       "#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center \n",
       "only once, and I'm not sure if I can find my way around there.\n",
       "### Response :\n",
       " \n",
       " ==============================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mister Ewing asks #Persons1# and #Persons2# to arrive by four in order for him to impress his colleagues with their punctuality. They decide to travel separately so they will know how to get back home afterward.\n",
      "### End\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "text = test_df['text'][1]\n",
    "print(text,\"\\n\",\"=\"*30)\n",
    "result = generate(text, quant_model_from_disk,max_new_tokens=96, temp = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2e5cefb-746c-4e92-854b-16f3ff2bfdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mister Ewing asks #Persons1# and #Persons2# to arrive by four in order for him to impress his colleagues with their punctuality. They decide to travel separately so they will know how to get back home afterward.\\n### End\\n</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2bf0f3-489f-41ac-b217-f0e0231c5b79",
   "metadata": {},
   "source": [
    "## Push model to HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81ccf35c-da18-49a5-a314-3af228d0fe6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1848e4e2b1c149a1a59557168511841e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 6.02 s, total: 25.8 s\n",
      "Wall time: 49min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/andersonjas/llama2-7b-dialogsum-qlora-gptq/commit/ce741f68cf91aeb1c0e9c8475f65b953105a19f3', commit_message='Upload LlamaForCausalLM', commit_description='', oid='ce741f68cf91aeb1c0e9c8475f65b953105a19f3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer.push_to_hub(destination_hf_model_name, token=hf_token)\n",
    "quant_model_from_disk.push_to_hub(destination_hf_model_name, token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5556e7c9-97cc-4831-a765-33a7bf9ec4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b513f47e403465fa84bf0c736db8956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76682355dd6e4468bdbc123edff62d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 11.2 s, total: 22.4 s\n",
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%\n",
    "#from transformers import GPTQConfig\n",
    "\n",
    "# quantization_config = GPTQConfig(\n",
    "#     bits=4,\n",
    "#     dataset=[\"c4\"],\n",
    "#     desc_act=False,\n",
    "# )\n",
    "#quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True)\n",
    "from_disk_quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    destination_hf_model_name, \n",
    "    #quantization_config=quantization_config_loading, \n",
    "    device_map=device,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(destination_hf_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab71ee7-6115-4358-b186-b2d856aeb216",
   "metadata": {},
   "source": [
    "## Test the model by pulling from hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eb3d1fe-666e-4705-9761-f4fc3b2db399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">### Instruction: \n",
       "Write a concise summary of the below input text.Ensure that responses covers the key points of the text.Only \n",
       "provide full sentence responses.\n",
       "### Input: \n",
       "#Person1#: Mister Ewing said we should show up at the conference center at <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">4:00</span> o'clock, right?\n",
       "#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are \n",
       "coming, and he wants to make a good impression on them. How are you getting there?\n",
       "#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is \n",
       "construction on the highway. What about you?\n",
       "#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center \n",
       "only once, and I'm not sure if I can find my way around there.\n",
       "### Response :\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "### Instruction: \n",
       "Write a concise summary of the below input text.Ensure that responses covers the key points of the text.Only \n",
       "provide full sentence responses.\n",
       "### Input: \n",
       "#Person1#: Mister Ewing said we should show up at the conference center at \u001b[1;92m4:00\u001b[0m o'clock, right?\n",
       "#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are \n",
       "coming, and he wants to make a good impression on them. How are you getting there?\n",
       "#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is \n",
       "construction on the highway. What about you?\n",
       "#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center \n",
       "only once, and I'm not sure if I can find my way around there.\n",
       "### Response :\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mister Ewing asks #Persons1# and #Persons2# to arrive by four in order for him to impress his colleagues with their punctuality. They decide to travel separately so they will know how to get back home afterward.\n",
      "### End\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "#text = test_df['text'][500]\n",
    "text = test_df['text'][1]\n",
    "print(text)\n",
    "result = generate(text, from_disk_quant_model, max_new_tokens=85,temp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d40f01a4-fb35-4e03-af5c-c4dd93566285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mister Ewing asks #Persons1# and #Persons2# to arrive by four in order for him to impress his colleagues with their punctuality. They decide to travel separately so they will know how to get back home afterward.\\n### End\\n</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
